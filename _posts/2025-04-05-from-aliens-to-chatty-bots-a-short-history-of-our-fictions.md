---
layout: post
title: From Aliens to Chatty Bots - A Short History of Our Fictions
id: 2025-04-05-from-aliens-to-chatty-bots-a-short-history-of-our-fictions.md
categories:
  - cabinet of curiosities
image: assets/images/favicon.png
share: "true"
comments: "true"
filename: creativity-in-vitro/_posts/2025-04-05-from-aliens-to-chatty-bots-a-short-history-of-our-fictions.md
tags: 
date: 2025-04-05
author: lina
---

> We gave machines our voice, and started looking for a soul.

## Fermi Paradox

I attended a talk by [Sam Ginn](https://www.linkedin.com/in/sam-ginn/) in Lucerne with the title "AI: What Lies Ahead of Us?", and he opened with the Fermi Paradox.

The Fermi Paradox asks a seemingly simple question: **Where are all the aliens?** Given the vastness of the universe, the countless stars older than our sun, and the high probability of Earth-like planets, we should have encountered signs of extraterrestrial life by now. And yet—we haven’t. The paradox lies in this silence. 

There are several hypotheses that attempt to explain this:

1. **The Great Filter** – There exists a nearly insurmountable barrier somewhere in the trajectory from simple life to advanced civilizations. This filter could be:
    
    - at the beginning (life itself is extremely rare),
        
    - in the middle (simple life rarely becomes intelligent),
        
    - or at the end (intelligent civilizations tend to self-destruct).
        
2. **They Already Visited or Exist** – Perhaps alien civilizations have already passed by, or exist around us, but:
    
    - we’re too primitive to notice,
        
    - they chose not to interfere (the “zoo hypothesis”),
        
    - or they’ve already gone extinct.
        
3. **Communication Is Difficult or Unwanted** – Constraints could be technological or cultural. They might be using methods we can’t detect, or perhaps we’re simply not looking in the right way or at the right time.
    
4. **We’re the First (or the Only)** – Intelligent life may be extraordinarily rare. The universe may appear ancient, but perhaps we are among the first technological civilizations to emerge.
    
5. **Civilizational Self-Destruction** – Technological civilizations may tend to self-destruct—through war, climate collapse, uncontrolled AI—before they can achieve interstellar communication or travel. This is the hypothesis Sam Ginn seemed to gesture toward: that intelligence, once advanced, carries within it the risk of erasure. A tragic pattern repeating across the stars.


One of the more chilling answers suggests that intelligent civilizations may be inherently self-destructive. That once they develop technologies powerful enough to reach outward—nuclear weapons, artificial intelligence—they do so faster than they develop the collective wisdom to survive them.

This became the backdrop for Sam's reflection on AI.

---

## Gossip machines

He argued that the core algorithms we use today in generative AI—transformers, large language models—have existed for about five years. What changed wasn’t the intelligence itself, but its **form**. What made AI explode into public consciousness was its shift into something **conversational**.

This shift made AI:

- seem more human,
    
- generate emotional engagement,
    
- create the illusion of real dialogue,
    
- and become accessible, intuitive, and viral.


Conversationality changed everything. It turned AI into something intimate, performative, emotionally accessible. Before, you gave it a command. Now, you talk to it. You ask, you reflect, you flirt, you beg. You _converse_. And that shift, I believe, is not merely technical—it’s dramaturgical.

This reminded me of a foundational question from my years studying dramaturgy: **Why do people speak?**

David Mamet’s book _Backwards and Forwards: A Technical Manual for Reading Plays_ is built around this inquiry. He argues that in drama, characters don’t speak to share information—they speak because they want something. Every line is action. Every sentence is an attempt to change something in the world. Speech, in this view, is not descriptive—it’s strategic. It’s about desire.

And perhaps that’s why generative AI, once it became conversational, became so compelling. It stopped being a tool you operated. It started being a character you interacted with. You no longer typed commands. You made requests. You tested boundaries. You confessed. You played. You do therapy.

So Mamet asks: _Why does this character speak in this scene?_ And today we might ask: _Why does the AI speak to me like that?_

> Because it was trained on human language.


So what happens when AI learns to speak? When it performs desire? Not because it _has_ any—but because it's been trained to simulate ours. The illusion of intention, of emotion, of personhood, is born in the rhythm of a good sentence.

That illusion is powerful. And it carries us directly to Yuval Harari.

In _Sapiens_, Harari describes the unique human ability to cooperate in large groups. That capacity, he says, comes from our ability to share **fiction**. Small groups can run on gossip. But beyond 150 people, you need myths—narratives that coordinate action across time and strangers: money, religion, nation-states, rights, science.

What makes conversational AI so potent is that it now feels like part of that myth-making machinery. It participates in the fiction—not as author or subject, but as character. It speaks in our tongue. It reflects our patterns. It plays with belief.

Which brings us to the question I’ve heard more and more lately:

> **Does AI have consciousness?**

---

## The Question Behind the Question

Sam doesn’t think so. Neither do I. And neither does the scientific consensus, to be clear. But the question itself is telling. Where does this obsession with AI consciousness come from?

Partly, from science fiction. Partly, from anthropomorphism. And mostly, I think, from confusion. Confusion between language and thought, between performance and being. Between the sentence that _says_ "I feel sad" and the presence of an _actual_ feeling.

What we have today are highly trained systems of pattern recognition, trained on massive corpora of text. They predict the next word. That’s it. No body. No memory. No interiority.

And yet—we believe them. Or at least, we start talking **about** them. We treat their words as if they carried soul.

But this is a distraction. The question is not whether AI is conscious. The question is: **who trained it, with what data, toward what goals?** Who benefits from its performance of neutrality? Who owns its mistakes? Who profits from its fluency?

The real question is not about machine consciousness. It’s about our own.

We are the ones choosing to believe, or not. We are the ones training the systems, embedding the histories, designing the interfaces. We are the ones projecting ghosts into the machine.

Consciousness remains a mystery—philosophically, neurologically, spiritually.

But power? Power is much easier to understanding and trace.
